"""
========================================
Track 1: Yahoo Finance API ë‰´ìŠ¤ ë§í¬ ìˆ˜ì§‘
========================================

ëª©ì :
- NASDAQ ì¢…ëª©ì˜ ë‰´ìŠ¤ ë§í¬ ìˆ˜ì§‘
- Yahoo Finance Search API ì‚¬ìš© (v1)
- ì¢…ëª© ì„ íƒ ë° ê°œìˆ˜ ì„¤ì • ê°€ëŠ¥
- ì¤‘ë³µ ì œê±° (URL ê¸°ì¤€)

ì‚¬ìš©ë²•:
- ì „ì²´ ì¢…ëª©: python news_api_collector.py
- ì „ì²´ ì¢…ëª© + ê°œìˆ˜: python news_api_collector.py --count 5
- íŠ¹ì • ì¢…ëª©: python news_api_collector.py --symbols AAPL,MSFT,GOOGL
- íŠ¹ì • ì¢…ëª© + ê°œìˆ˜: python news_api_collector.py --symbols AAPL,MSFT --count 3

ì¶œë ¥:
- python/output/news_links.json
"""

import requests
import json
from datetime import datetime
import pytz
from pathlib import Path
import logging
from concurrent.futures import ThreadPoolExecutor, as_completed
import pandas as pd
import argparse
import time

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

kst = pytz.timezone('Asia/Seoul')


class NewsConfig:
    """ì„¤ì •"""
    DEFAULT_NEWS_PER_SYMBOL = 5  # ê¸°ë³¸ê°’: ì¢…ëª©ë‹¹ 5ê°œ
    MAX_NEWS_PER_SYMBOL = 10     # ìµœëŒ€: ì¢…ëª©ë‹¹ 10ê°œ
    MAX_WORKERS = 10             # ë³‘ë ¬ ì²˜ë¦¬ ê°œìˆ˜
    RETRY_COUNT = 2              # ì¬ì‹œë„ íšŸìˆ˜
    RETRY_DELAY = 1              # ì¬ì‹œë„ ëŒ€ê¸° ì‹œê°„ (ì´ˆ)
    REQUEST_DELAY = 0.1          # ìš”ì²­ ê°„ ë”œë ˆì´ (ì´ˆ)
    OUTPUT_DIR = 'python/output'
    OUTPUT_FILE = 'news_links.json'
    
    # Yahoo Finance Search API
    API_URL = "https://query1.finance.yahoo.com/v1/finance/search"
    HEADERS = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
    }


def load_symbols_from_csv():
    """CSVì—ì„œ 101ê°œ ì¢…ëª© ë¡œë“œ"""
    try:
        csv_file = 'python/nasdaq100_tickers.csv'
        
        if not Path(csv_file).exists():
            csv_file = 'nasdaq100_tickers.csv'
        
        df = pd.read_csv(csv_file)
        symbols = df['symbol'].str.strip().str.upper().tolist()
        
        logger.info(f"ğŸ“‚ Loaded {len(symbols)} symbols from CSV")
        return symbols
        
    except Exception as e:
        logger.error(f"âŒ Failed to load CSV: {e}")
        raise


def fetch_news_for_symbol(symbol, max_news, retry_count=NewsConfig.RETRY_COUNT):
    """
    Yahoo Finance Search APIë¡œ ë‰´ìŠ¤ ìˆ˜ì§‘
    
    Returns:
        list: [{'symbol', 'title', 'url', 'summary', 'publisher', 'published_at', 'thumbnail_url'}]
    """
    for attempt in range(retry_count + 1):
        try:
            url = f"{NewsConfig.API_URL}?q={symbol}&newsCount={max_news}"
            
            response = requests.get(
                url, 
                headers=NewsConfig.HEADERS, 
                timeout=10
            )
            
            if response.status_code == 429:
                # Rate limit - ëŒ€ê¸° í›„ ì¬ì‹œë„
                if attempt < retry_count:
                    logger.warning(f"  [RETRY] {symbol}: Rate limited, waiting...")
                    time.sleep(NewsConfig.RETRY_DELAY * 2)
                    continue
                else:
                    logger.error(f"  [ERROR] {symbol}: Rate limited after {retry_count} retries")
                    return []
            
            if response.status_code != 200:
                logger.error(f"  [ERROR] {symbol}: HTTP {response.status_code}")
                return []
            
            data = response.json()
            news_list = data.get('news', [])
            
            if not news_list:
                logger.debug(f"  [SKIP] {symbol}: No news")
                return []
            
            processed_news = []
            
            for article in news_list[:max_news]:
                try:
                    # ë°œí–‰ ì‹œê°„ (Unix timestamp â†’ KST)
                    publish_timestamp = article.get('providerPublishTime', 0)
                    if publish_timestamp:
                        dt = datetime.fromtimestamp(publish_timestamp, tz=pytz.UTC)
                        dt_kst = dt.astimezone(kst)
                        published_at = dt_kst.strftime('%Y-%m-%d %H:%M:%S')
                    else:
                        published_at = datetime.now(kst).strftime('%Y-%m-%d %H:%M:%S')
                    
                    # ì¸ë„¤ì¼ URL
                    thumbnail_url = None
                    thumbnail = article.get('thumbnail', {})
                    if thumbnail and 'resolutions' in thumbnail:
                        resolutions = thumbnail['resolutions']
                        if resolutions:
                            # 140x140 ë˜ëŠ” ì²« ë²ˆì§¸ ì´ë¯¸ì§€
                            for res in resolutions:
                                if res.get('tag') == '140x140':
                                    thumbnail_url = res.get('url')
                                    break
                            if not thumbnail_url:
                                thumbnail_url = resolutions[0].get('url')
                    
                    news_item = {
                        'symbol': symbol,
                        'title': article.get('title', 'No Title'),
                        'url': article.get('link', ''),
                        'summary': '',  # Search APIëŠ” summary ë¯¸ì œê³µ
                        'publisher': article.get('publisher', 'Unknown'),
                        'published_at': published_at,
                        'thumbnail_url': thumbnail_url
                    }
                    
                    if news_item['url']:
                        processed_news.append(news_item)
                    
                except Exception as e:
                    logger.debug(f"  [WARN] {symbol}: Failed to process article: {e}")
                    continue
            
            if processed_news:
                logger.info(f"  [OK] {symbol}: {len(processed_news)} news")
            
            return processed_news
            
        except requests.exceptions.Timeout:
            if attempt < retry_count:
                logger.warning(f"  [RETRY] {symbol}: Timeout, retrying ({attempt + 1}/{retry_count})...")
                time.sleep(NewsConfig.RETRY_DELAY)
            else:
                logger.error(f"  [ERROR] {symbol}: Timeout after {retry_count} retries")
                return []
                
        except Exception as e:
            if attempt < retry_count:
                logger.warning(f"  [RETRY] {symbol}: {e}, retrying ({attempt + 1}/{retry_count})...")
                time.sleep(NewsConfig.RETRY_DELAY)
            else:
                logger.error(f"  [ERROR] {symbol}: {e}")
                return []
    
    return []


def collect_all_news_parallel(symbols, max_workers, max_news):
    """ë³‘ë ¬ë¡œ ë‰´ìŠ¤ ë§í¬ ìˆ˜ì§‘"""
    logger.info("="*60)
    logger.info(f"[PARALLEL] Starting with {max_workers} workers")
    logger.info(f"[CONFIG] Symbols: {len(symbols)}, News per symbol: {max_news}")
    logger.info("="*60)
    
    all_news = []
    success_count = 0
    error_count = 0
    
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        future_to_symbol = {
            executor.submit(fetch_news_for_symbol, symbol, max_news): symbol 
            for symbol in symbols
        }
        
        for future in as_completed(future_to_symbol):
            symbol = future_to_symbol[future]
            try:
                news_list = future.result()
                if news_list:
                    all_news.extend(news_list)
                    success_count += 1
                else:
                    error_count += 1
            except Exception as e:
                logger.error(f"  [ERROR] {symbol}: {e}")
                error_count += 1
            
            # Rate limit ë°©ì§€ë¥¼ ìœ„í•œ ë”œë ˆì´
            time.sleep(NewsConfig.REQUEST_DELAY)
    
    logger.info("="*60)
    logger.info(f"[STATS] Success: {success_count}, Errors: {error_count}")
    logger.info(f"[STATS] Total news: {len(all_news)}")
    logger.info("="*60)
    
    return all_news


def remove_duplicates(news_list):
    """URL ê¸°ì¤€ìœ¼ë¡œ ì¤‘ë³µ ì œê±°"""
    seen_urls = set()
    unique_news = []
    
    for news in news_list:
        url = news['url']
        if url not in seen_urls:
            seen_urls.add(url)
            unique_news.append(news)
    
    removed = len(news_list) - len(unique_news)
    logger.info(f"[DEDUP] {len(news_list)} â†’ {len(unique_news)} (removed {removed} duplicates)")
    
    return unique_news


def save_news_links_to_json(news_list, output_dir, output_file):
    """ë‰´ìŠ¤ ë§í¬ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
    try:
        Path(output_dir).mkdir(parents=True, exist_ok=True)
        
        output_path = Path(output_dir) / output_file
        
        output_data = {
            'timestamp': datetime.now(kst).strftime('%Y-%m-%d %H:%M:%S'),
            'total_news': len(news_list),
            'data': news_list
        }
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, indent=2, ensure_ascii=False)
        
        logger.info("="*60)
        logger.info(f"[SAVE] News links saved: {output_path}")
        logger.info(f"[STATS] Total news: {len(news_list)}")
        logger.info("="*60)
        
        return str(output_path)
        
    except Exception as e:
        logger.error(f"[ERROR] Failed to save news links: {e}")
        raise


def parse_arguments():
    """ëª…ë ¹ì¤„ ì¸ì íŒŒì‹±"""
    parser = argparse.ArgumentParser(description='Yahoo Finance ë‰´ìŠ¤ ìˆ˜ì§‘ê¸°')
    
    parser.add_argument(
        '--symbols',
        type=str,
        default='',
        help='ìˆ˜ì§‘í•  ì¢…ëª© (ì‰¼í‘œ êµ¬ë¶„). ë¹„ì›Œë‘ë©´ ì „ì²´ ì¢…ëª©'
    )
    
    parser.add_argument(
        '--count',
        type=int,
        default=NewsConfig.DEFAULT_NEWS_PER_SYMBOL,
        help=f'ì¢…ëª©ë‹¹ ë‰´ìŠ¤ ê°œìˆ˜ (ê¸°ë³¸: {NewsConfig.DEFAULT_NEWS_PER_SYMBOL}, ìµœëŒ€: {NewsConfig.MAX_NEWS_PER_SYMBOL})'
    )
    
    return parser.parse_args()


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    # ì¸ì íŒŒì‹±
    args = parse_arguments()
    
    # ì¢…ëª© ê²°ì •
    if args.symbols:
        symbols = [s.strip().upper() for s in args.symbols.split(',') if s.strip()]
        logger.info(f"ğŸ“Œ íŠ¹ì • ì¢…ëª© ëª¨ë“œ: {symbols}")
    else:
        symbols = load_symbols_from_csv()
        logger.info(f"ğŸ“Œ ì „ì²´ ì¢…ëª© ëª¨ë“œ: {len(symbols)}ê°œ")
    
    # ê°œìˆ˜ ì œí•œ
    max_news = min(args.count, NewsConfig.MAX_NEWS_PER_SYMBOL)
    max_news = max(max_news, 1)  # ìµœì†Œ 1ê°œ
    
    logger.info("="*60)
    logger.info("Track 1: News API Collector Started")
    logger.info("="*60)
    logger.info(f"Configuration:")
    logger.info(f"  - Symbols: {len(symbols)}")
    logger.info(f"  - News per symbol: {max_news}")
    logger.info(f"  - Max workers: {NewsConfig.MAX_WORKERS}")
    logger.info(f"  - API: Yahoo Finance Search API (v1)")
    logger.info("="*60)
    
    try:
        # 1. ë‰´ìŠ¤ ë§í¬ ìˆ˜ì§‘ (ë³‘ë ¬)
        news_list = collect_all_news_parallel(
            symbols, 
            max_workers=NewsConfig.MAX_WORKERS,
            max_news=max_news
        )
        
        # 2. ì¤‘ë³µ ì œê±°
        unique_news = remove_duplicates(news_list)
        
        # 3. JSON ì €ì¥
        save_news_links_to_json(
            unique_news,
            output_dir=NewsConfig.OUTPUT_DIR,
            output_file=NewsConfig.OUTPUT_FILE
        )
        
        logger.info("="*60)
        logger.info("âœ… Track 1 Completed Successfully")
        logger.info("="*60)
        
    except Exception as e:
        logger.error(f"âŒ Track 1 failed: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
